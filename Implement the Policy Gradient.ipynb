{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "219dc23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode training complete.\n",
      "Episode: 1 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 2 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 3 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 4 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 5 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 6 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 7 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 8 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 9 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 10 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 11 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 12 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 13 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 14 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 15 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 16 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 17 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 18 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 19 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 20 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 21 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 22 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 23 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 24 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 25 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 26 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 27 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 28 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 29 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 30 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 31 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 32 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 33 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 34 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 35 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 36 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 37 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 38 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 39 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 40 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 41 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 42 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 43 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 44 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 45 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 46 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 47 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 48 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 49 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 50 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 51 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 52 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 53 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 54 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 55 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 56 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 57 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 58 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 59 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 60 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 61 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 62 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 63 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 64 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 65 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 66 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 67 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 68 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 69 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 70 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 71 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 72 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 73 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 74 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 75 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 76 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 77 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 78 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 79 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 80 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 81 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 82 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 83 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 84 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 85 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 86 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 87 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 88 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 89 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 90 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 91 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 92 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 93 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 94 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 95 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 96 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 97 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 98 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 99 Total Reward: 1\n",
      "Episode training complete.\n",
      "Episode: 100 Total Reward: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #package \n",
    "\n",
    "class REINFORCEAgent:# class\n",
    "    def __init__(self, num_actions, num_states, gamma=0.99, learning_rate=0.01):\n",
    "         # gamma is discount factor for finding future reward\n",
    "        self.num_actions = num_actions\n",
    "        self.num_states = num_states\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.policy = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    def get_action(self, state): #return action for current policy\n",
    "        action_probs = self._softmax(self.policy[state])\n",
    "        #Accesses the policy for the current state\n",
    "        #softmax function that takes as input a vector of real numbers and returns a vector of probabilities\n",
    "        return np.random.choice(self.num_actions, p=action_probs)\n",
    "    \n",
    "    def train(self, episode):\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        returns = self._calculate_returns(rewards)\n",
    "        for t, (state, action) in enumerate(zip(states, actions)):\n",
    "            delta = returns[t] - self.policy[state, action]\n",
    "            self.policy[state, action] += self.learning_rate * delta\n",
    "        print(\"Episode training complete.\")\n",
    "            #enumerate built-in function that allows you to loop over an iterable (such as a list, tuple, or string)\n",
    "    def _calculate_returns(self, rewards):\n",
    "        G = 0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return returns\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        exp_values = np.exp(x - np.max(x)) #exponential \n",
    "        return exp_values / np.sum(exp_values)\n",
    "\n",
    "# Simple environment\n",
    "class SimpleEnvironment:# class\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def reset(self):\n",
    "        return 0\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        new_state = max(0, min(self.num_states - 1, state + (action * 2 - 1)))\n",
    "        reward = 1 if new_state == self.num_states - 1 else 0\n",
    "        return new_state, reward\n",
    "\n",
    "# Training loop\n",
    "num_states = 5\n",
    "num_actions = 2\n",
    "num_episodes = 100\n",
    "env = SimpleEnvironment(num_states, num_actions)\n",
    "agent = REINFORCEAgent(num_actions, num_states)\n",
    "\n",
    "for episode_num in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    total_reward = 0  # Track total reward for the episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward = env.step(state, action)\n",
    "        total_reward += reward  # Accumulate reward for the episode\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        done = next_state == num_states - 1\n",
    "    agent.train(episode)\n",
    "    print(\"Episode:\", episode_num + 1, \"Total Reward:\", total_reward)  # Print total reward for the episode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8a15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
